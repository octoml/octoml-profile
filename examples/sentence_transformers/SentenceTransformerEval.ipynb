{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60621bcb",
   "metadata": {},
   "source": [
    "# Benchmark Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ab5fc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will show how you can use `octoml-profile` to quickly benchmark different SentenceTransformers on various hardware software backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5def1546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List\n",
    "from octoml_profile import accelerate, remote_profile\n",
    "from octoml_profile.report import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebf647",
   "metadata": {},
   "source": [
    "## Step 1: Set the necessary environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a6bfea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# os.environ['OCTOML_PROFILE_API_TOKEN'] = \"REPLACE ME\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4723f",
   "metadata": {},
   "source": [
    "## Step 2: Define what API we want to measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c432ba",
   "metadata": {},
   "source": [
    "This is the API we are interested in benchmarking. It takes a list of query strings, and a precomputed corpus embeddings, returns the topk similar document per query string, and their score.\n",
    "\n",
    "The magic `accelerate` decorator will be used later to run and benchmark tensor programs remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b42fc48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "@accelerate\n",
    "def semantic_search(model, queries: List[str], corpus_embeddings: List[torch.Tensor], topk: int):\n",
    "    \"\"\"Example from https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "    \"\"\"\n",
    "    topk = min(topk, len(corpus))\n",
    "    query_embedding = model.encode(queries, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)\n",
    "    topk_scores, topk_index = torch.topk(cos_scores, k=topk, dim=-1)\n",
    "    return (topk_scores, topk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e7ca0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(model_id: str, queries: List[str], corpus: List[str]):\n",
    "    print(\"======================\")\n",
    "    print(\"Model:\", model_id)\n",
    "    model = SentenceTransformer(model_id)\n",
    "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    # run two times and the initial run with compilation will be discarded\n",
    "    for _ in range(2):\n",
    "        scores, indices = semantic_search(model, queries, corpus_embeddings, topk=5)\n",
    "    for query_id, query in enumerate(queries):\n",
    "        print(\"\\nQuery:\", query)\n",
    "        print(\"Top 5 most similar sentences in corpus:\")\n",
    "        for score, doc_id in zip(scores[query_id], indices[query_id]):\n",
    "            print(corpus[doc_id], \"(Score: {:.4f})\".format(score))\n",
    "    print(\"======================\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "716bb624",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Remote Benchmark Code\n",
    "\n",
    "To run the same code above but on remote hardware/software backend, we simply need to wrap the `run_model` with a `remote_profile` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9943ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BenchmarkRecord = namedtuple('Record', ['model', 'backend', 'time_ms', 'cost_per_mreq', 'batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d61256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remote_benchmark(model_id, queries, corpus, batch_size, backends=None, verbose=True) -> List[BenchmarkRecord]:\n",
    "    # Fill batches queries from repeatedly adding queries up to length batch_size\n",
    "    q, r = divmod(batch_size, len(queries))\n",
    "    batched_queries =  q * queries + queries[:r]\n",
    "\n",
    "    with remote_profile(backends=backends, print_results_to=sys.stdout if verbose else None) as r:\n",
    "        run_model(model_id, batched_queries, corpus)\n",
    "        return parse_report(model_id, r.report(), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c98fcab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_report(model_id, report: ProfileReport, batch_size) -> List[BenchmarkRecord]:\n",
    "    \"\"\"Helper function to parse the profiling report\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    hw_cost = {\n",
    "        'r6i.large': 0.126,\n",
    "        'r7g.large': 0.107,\n",
    "        'g4dn.xlarge': 0.526,\n",
    "        'g5.xlarge': 1.006\n",
    "    }\n",
    "    assert len(report.profiles) == 1\n",
    "    uncompiled_code_ms = report.profiles[0].total_uncompiled_ms\n",
    "    report.profiles[0].print()\n",
    "    for backend, result in report.profiles[0].total_per_backend.items():\n",
    "        if len(result.errors) > 0:\n",
    "            message = \"\\n\".join(result.errors)\n",
    "            raise RuntimeError(f\"Error in running {model_id} on {backend}: {message}\")\n",
    "        total_time_ms = uncompiled_code_ms + result.estimated_total_ms\n",
    "        cost_per_hr = hw_cost[backend.split(\"/\")[0]]\n",
    "        cost_per_mreq = cost_per_hr * (1e6 * total_time_ms / (3600 * 1000))\n",
    "        records.append(BenchmarkRecord(model_id, backend, total_time_ms, cost_per_mreq, batch_size))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579fd3a",
   "metadata": {},
   "source": [
    "### Step 4: Set the data and try remote benchmark with one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e5d85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = ['A man is eating pasta.',\n",
    "           'Someone in a gorilla costume is playing a set of drums.',\n",
    "           'A cheetah chases prey on across a field.']\n",
    "\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d90b1",
   "metadata": {},
   "source": [
    "### Step 5: Let's evaluate on many models and many backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8303bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Models can be found from: https://www.sbert.net/docs/pretrained_models.html\n",
    "model_ids = ['all-MiniLM-L12-v2',\n",
    "             'all-MiniLM-L6-v2',\n",
    "             'all-distilroberta-v1',\n",
    "             'paraphrase-albert-small-v2',\n",
    "             'paraphrase-MiniLM-L3-v2',\n",
    "             ]\n",
    "# Backends can be found from `session.supported_backends()`\n",
    "backends = ['r6i.large/onnxrt-cpu',\n",
    "            'r6i.large/torch-eager-cpu',\n",
    "            'r7g.large/onnxrt-cpu',\n",
    "            'g4dn.xlarge/onnxrt-cuda',\n",
    "            'g4dn.xlarge/onnxrt-tensorrt',\n",
    "            'g4dn.xlarge/torch-eager-cuda',\n",
    "            'g4dn.xlarge/torch-inductor-cuda',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e265d712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def benchmark_all(model_ids, backends, output_file, batch_size):\n",
    "    records = []\n",
    "    for model_id in model_ids:\n",
    "        results = remote_benchmark(model_id,\n",
    "                                   queries,\n",
    "                                   corpus,\n",
    "                                   batch_size,\n",
    "                                   backends=backends,\n",
    "                                   verbose=False)\n",
    "        records.extend(results)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b75f3a20-4066-4700-9e99-a55bdcc1ce9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Model: all-MiniLM-L12-v2\n",
      "> \u001b[0;32m/home/anwang/dynamite/client/octoml_profile/client.py\u001b[0m(533)\u001b[0;36m_request_stream_from_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    531 \u001b[0;31m            \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    532 \u001b[0;31m            \u001b[0;31m# Disable interactive progress for Jupyter with ipykernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 533 \u001b[0;31m            \u001b[0minteractive_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    534 \u001b[0;31m        \u001b[0mMAX_CHUNK_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    535 \u001b[0;31m        \u001b[0mnum_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "UnsupportedOperation('fileno')\n",
      "fileno\n",
      "<class 'io.UnsupportedOperation'>\n",
      "True\n",
      "*** SyntaxError: unexpected EOF while parsing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/client.py:424\u001b[0m, in \u001b[0;36mRemoteInferenceSession.load_model\u001b[0;34m(self, model_id, backend_ids, model_format, model_files, input_names, output_names)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mstub\u001b[39m.\u001b[39;49mLoadCachedModelComponent(req)\n\u001b[1;32m    425\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded [item \u001b[39m\u001b[39m{\u001b[39;00mmodel_component_id\u001b[39m}\u001b[39;00m\u001b[39m of graph \u001b[39m\u001b[39m{\u001b[39;00mmodel_id\u001b[39m}\u001b[39;00m\u001b[39m] from cache\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    426\u001b[0m           file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:247\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    241\u001b[0m              request: Any,\n\u001b[1;32m    242\u001b[0m              timeout: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m              wait_for_ready: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m              compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 247\u001b[0m     response, ignored_call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_call(request,\n\u001b[1;32m    248\u001b[0m                                              timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    249\u001b[0m                                              metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    250\u001b[0m                                              credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[1;32m    251\u001b[0m                                              wait_for_ready\u001b[39m=\u001b[39;49mwait_for_ready,\n\u001b[1;32m    252\u001b[0m                                              compression\u001b[39m=\u001b[39;49mcompression)\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:290\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    287\u001b[0m call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interceptor\u001b[39m.\u001b[39mintercept_unary_unary(continuation,\n\u001b[1;32m    288\u001b[0m                                                client_call_details,\n\u001b[1;32m    289\u001b[0m                                                request)\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49mresult(), call\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_channel.py:343\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:274\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     response, call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_thunk(new_method)\u001b[39m.\u001b[39;49mwith_call(\n\u001b[1;32m    275\u001b[0m         request,\n\u001b[1;32m    276\u001b[0m         timeout\u001b[39m=\u001b[39;49mnew_timeout,\n\u001b[1;32m    277\u001b[0m         metadata\u001b[39m=\u001b[39;49mnew_metadata,\n\u001b[1;32m    278\u001b[0m         credentials\u001b[39m=\u001b[39;49mnew_credentials,\n\u001b[1;32m    279\u001b[0m         wait_for_ready\u001b[39m=\u001b[39;49mnew_wait_for_ready,\n\u001b[1;32m    280\u001b[0m         compression\u001b[39m=\u001b[39;49mnew_compression)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:301\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwith_call\u001b[39m(\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    294\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    300\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, grpc\u001b[39m.\u001b[39mCall]:\n\u001b[0;32m--> 301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_call(request,\n\u001b[1;32m    302\u001b[0m                            timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    303\u001b[0m                            metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    304\u001b[0m                            credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[1;32m    305\u001b[0m                            wait_for_ready\u001b[39m=\u001b[39;49mwait_for_ready,\n\u001b[1;32m    306\u001b[0m                            compression\u001b[39m=\u001b[39;49mcompression)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:290\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    287\u001b[0m call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interceptor\u001b[39m.\u001b[39mintercept_unary_unary(continuation,\n\u001b[1;32m    288\u001b[0m                                                client_call_details,\n\u001b[1;32m    289\u001b[0m                                                request)\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49mresult(), call\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_channel.py:343\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:274\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     response, call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_thunk(new_method)\u001b[39m.\u001b[39;49mwith_call(\n\u001b[1;32m    275\u001b[0m         request,\n\u001b[1;32m    276\u001b[0m         timeout\u001b[39m=\u001b[39;49mnew_timeout,\n\u001b[1;32m    277\u001b[0m         metadata\u001b[39m=\u001b[39;49mnew_metadata,\n\u001b[1;32m    278\u001b[0m         credentials\u001b[39m=\u001b[39;49mnew_credentials,\n\u001b[1;32m    279\u001b[0m         wait_for_ready\u001b[39m=\u001b[39;49mnew_wait_for_ready,\n\u001b[1;32m    280\u001b[0m         compression\u001b[39m=\u001b[39;49mnew_compression)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_channel.py:957\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    955\u001b[0m state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    956\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m--> 957\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_channel.py:849\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"No such sha256 found in cache\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:35.81.35.43:443 {grpc_message:\"No such sha256 found in cache\", grpc_status:5, created_time:\"2023-03-22T11:53:21.510328613-07:00\"}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m result_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msentence_transformer_eval_with_torchxxxx.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(result_file):\n\u001b[0;32m----> 4\u001b[0m     records \u001b[39m=\u001b[39m benchmark_all(model_ids, backends, result_file, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     records256 \u001b[39m=\u001b[39m benchmark_all(model_ids, backends, result_file, \u001b[39m256\u001b[39m)\n\u001b[1;32m      7\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mrecords \u001b[39m+\u001b[39m records256)\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mbenchmark_all\u001b[0;34m(model_ids, backends, output_file, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m records \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_id \u001b[39min\u001b[39;00m model_ids:\n\u001b[0;32m----> 4\u001b[0m     results \u001b[39m=\u001b[39m remote_benchmark(model_id,\n\u001b[1;32m      5\u001b[0m                                queries,\n\u001b[1;32m      6\u001b[0m                                corpus,\n\u001b[1;32m      7\u001b[0m                                batch_size,\n\u001b[1;32m      8\u001b[0m                                backends\u001b[39m=\u001b[39;49mbackends,\n\u001b[1;32m      9\u001b[0m                                verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m     records\u001b[39m.\u001b[39mextend(results)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m records\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mremote_benchmark\u001b[0;34m(model_id, queries, corpus, batch_size, backends, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m batched_queries \u001b[39m=\u001b[39m  q \u001b[39m*\u001b[39m queries \u001b[39m+\u001b[39m queries[:r]\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m remote_profile(backends\u001b[39m=\u001b[39mbackends, print_results_to\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstdout \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mas\u001b[39;00m r:\n\u001b[0;32m----> 7\u001b[0m     run_model(model_id, batched_queries, corpus)\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m parse_report(model_id, r\u001b[39m.\u001b[39mreport(), batch_size)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model_id, queries, corpus)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# run two times and the initial run with compilation will be discarded\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     scores, indices \u001b[39m=\u001b[39m semantic_search(model, queries, corpus_embeddings, topk\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m query_id, query \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(queries):\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mQuery:\u001b[39m\u001b[39m\"\u001b[39m, query)\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/dynamo.py:702\u001b[0m, in \u001b[0;36maccelerate.<locals>.decorate.<locals>.decorated_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39mwith\u001b[39;00m _profile_run():\n\u001b[1;32m    701\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 702\u001b[0m         \u001b[39mreturn\u001b[39;00m profiling_dynamo_decorated(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m     \u001b[39mexcept\u001b[39;00m torchdynamo\u001b[39m.\u001b[39mexc\u001b[39m.\u001b[39mTorchDynamoException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    704\u001b[0m         exception \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(e)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:254\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    255\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     set_eval_frame(prior)\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36msemantic_search\u001b[0;34m(model, queries, corpus_embeddings, topk)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Example from https://www.sbert.net/examples/applications/semantic-search/README.html\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m topk \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(topk, \u001b[39mlen\u001b[39m(corpus))\n\u001b[0;32m----> 6\u001b[0m query_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(queries, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m cos_scores \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcos_sim(query_embedding, corpus_embeddings)\n\u001b[1;32m      8\u001b[0m topk_scores, topk_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(cos_scores, k\u001b[39m=\u001b[39mtopk, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:134\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, sentences: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m    112\u001b[0m            batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m,\n\u001b[1;32m    113\u001b[0m            show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m            device: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    118\u001b[0m            normalize_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[List[Tensor], ndarray, Tensor]:\n\u001b[1;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m    Computes sentence embeddings\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m       By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m show_progress_bar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         show_progress_bar \u001b[39m=\u001b[39m (logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mINFO \u001b[39mor\u001b[39;00m logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:136\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m show_progress_bar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     show_progress_bar \u001b[39m=\u001b[39m (logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mINFO \u001b[39mor\u001b[39;00m logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m convert_to_tensor:\n\u001b[1;32m    139\u001b[0m     convert_to_numpy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:136\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m show_progress_bar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     show_progress_bar \u001b[39m=\u001b[39m (logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mINFO \u001b[39mor\u001b[39;00m logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m convert_to_tensor:\n\u001b[1;32m    139\u001b[0m     convert_to_numpy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:146\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, batch_size, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    143\u001b[0m     convert_to_numpy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    145\u001b[0m input_was_string \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sentences, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(sentences, \u001b[39m'\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39m#Cast an individual sentence to a list with length 1\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[1;32m    148\u001b[0m     input_was_string \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:153\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, batch_size, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, show_progress_bar, input_was_string)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_device\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:156\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, batch_size, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, show_progress_bar, input_was_string)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[1;32m    157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:156\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, ___stack1, batch_size, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, show_progress_bar, input_was_string)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[1;32m    157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:162\u001b[0m, in \u001b[0;36m<resume in encode>\u001b[0;34m(___stack0, batch_size, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, show_progress_bar, input_was_string)\u001b[0m\n\u001b[1;32m    160\u001b[0m sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m    161\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(sentences_batch)\n\u001b[0;32m--> 162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:391\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 391\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:406\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    404\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    405\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    407\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:105\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    106\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:263\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    261\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 263\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    267\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    268\u001b[0m     compiler_fn,\n\u001b[1;32m    269\u001b[0m     one_graph,\n\u001b[1;32m    270\u001b[0m     export,\n\u001b[1;32m    271\u001b[0m     hooks,\n\u001b[1;32m    272\u001b[0m     frame,\n\u001b[1;32m    273\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/utils.py:164\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    163\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 164\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    166\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:326\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    325\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    327\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    328\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:530\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    527\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    528\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 530\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:313\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    300\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    301\u001b[0m     instructions,\n\u001b[1;32m    302\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    311\u001b[0m )\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m tracing(tracer\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 313\u001b[0m     tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    314\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    315\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1840\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1839\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1840\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:597\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    594\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    595\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    596\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 597\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    598\u001b[0m     ):\n\u001b[1;32m    599\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:560\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    559\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 560\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    562\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1919\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1914\u001b[0m _step_logger()(\n\u001b[1;32m   1915\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1916\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1917\u001b[0m )\n\u001b[1;32m   1918\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1919\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1920\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1921\u001b[0m )\n\u001b[1;32m   1922\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:569\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    566\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m count_calls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    568\u001b[0m     output\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 569\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, pass2\u001b[39m.\u001b[39;49mgraph_output_vars(), root)\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    572\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    573\u001b[0m         output\u001b[39m.\u001b[39mappend(pass2\u001b[39m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:615\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    612\u001b[0m name \u001b[39m=\u001b[39m unique_id(\u001b[39m\"\u001b[39m\u001b[39m__compiled_fn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    614\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[0;32m--> 615\u001b[0m compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    616\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    618\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/utils.py:164\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    163\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 164\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    166\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:697\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    695\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexample_inputs())\n\u001b[1;32m    696\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 697\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    698\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    699\u001b[0m \u001b[39massert\u001b[39;00m callable(compiled_fn), \u001b[39m\"\u001b[39m\u001b[39mcompiler_fn did not return callable\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1064\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1066\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/dynamo.py:359\u001b[0m, in \u001b[0;36m_safe_dynamo_backend\u001b[0;34m(_compile_func, gm, example_inputs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_dynamo_backend\u001b[39m(_compile_func: Callable, gm: torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mGraphModule,\n\u001b[1;32m    357\u001b[0m                          example_inputs: Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]):\n\u001b[1;32m    358\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         \u001b[39mreturn\u001b[39;00m _compile_func(gm, example_inputs)\n\u001b[1;32m    360\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    361\u001b[0m         ctx \u001b[39m=\u001b[39m _thread_local_state\u001b[39m.\u001b[39mprofiling_ctx\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/dynamo.py:542\u001b[0m, in \u001b[0;36m_remote_dynamo_backend\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m    540\u001b[0m loaded_infos \u001b[39m=\u001b[39m []\n\u001b[1;32m    541\u001b[0m \u001b[39mfor\u001b[39;00m loader, backend_indices \u001b[39min\u001b[39;00m _group_backends_by_loader(session\u001b[39m.\u001b[39mbackends):\n\u001b[0;32m--> 542\u001b[0m     loaded_info \u001b[39m=\u001b[39m loader(session, graph_info\u001b[39m.\u001b[39;49mgraph_id, backend_indices, gm, example_inputs)\n\u001b[1;32m    543\u001b[0m     loaded_infos\u001b[39m.\u001b[39mappend((loaded_info, backend_indices))\n\u001b[1;32m    545\u001b[0m active_input_mask \u001b[39m=\u001b[39m _get_overall_active_input_mask(loaded_infos, \u001b[39mlen\u001b[39m(example_inputs))\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/dynamo.py:419\u001b[0m, in \u001b[0;36m_load_onnx_model\u001b[0;34m(session, graph_id, backend_ids, gm, example_inputs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39mTemporaryDirectory() \u001b[39mas\u001b[39;00m dirpath:\n\u001b[1;32m    417\u001b[0m     onnx_model \u001b[39m=\u001b[39m _torchscript_to_onnx(torchscript, example_inputs, defaked_example_inputs,\n\u001b[1;32m    418\u001b[0m                                       input_names, output_names, dirpath)\n\u001b[0;32m--> 419\u001b[0m     session\u001b[39m.\u001b[39;49mload_model(model_id\u001b[39m=\u001b[39;49mgraph_id,\n\u001b[1;32m    420\u001b[0m                        backend_ids\u001b[39m=\u001b[39;49mbackend_ids,\n\u001b[1;32m    421\u001b[0m                        model_format\u001b[39m=\u001b[39;49mMODEL_FORMAT_ONNX,\n\u001b[1;32m    422\u001b[0m                        model_files\u001b[39m=\u001b[39;49monnx_model\u001b[39m.\u001b[39;49mget_file_descriptions(),\n\u001b[1;32m    423\u001b[0m                        input_names\u001b[39m=\u001b[39;49monnx_model\u001b[39m.\u001b[39;49mactive_input_names,\n\u001b[1;32m    424\u001b[0m                        output_names\u001b[39m=\u001b[39;49moutput_names)\n\u001b[1;32m    425\u001b[0m \u001b[39mreturn\u001b[39;00m _LoadedModelInfo(active_input_mask\u001b[39m=\u001b[39monnx_model\u001b[39m.\u001b[39mactive_input_mask)\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/client.py:432\u001b[0m, in \u001b[0;36mRemoteInferenceSession.load_model\u001b[0;34m(self, model_id, backend_ids, model_format, model_files, input_names, output_names)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mcode() \u001b[39m==\u001b[39m grpc\u001b[39m.\u001b[39mStatusCode\u001b[39m.\u001b[39mNOT_FOUND:\n\u001b[1;32m    429\u001b[0m     request_iter \u001b[39m=\u001b[39m _request_stream_from_file(\n\u001b[1;32m    430\u001b[0m         file_desc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session_uuid, model_id,\n\u001b[1;32m    431\u001b[0m         model_component_id, backend_skip_mask)\n\u001b[0;32m--> 432\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mstub\u001b[39m.\u001b[39;49mLoadModelComponent(request_iter)\n\u001b[1;32m    433\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[39mraise\u001b[39;00m LoadModelError(\u001b[39m\"\u001b[39m\u001b[39mError in load cached component: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mgraph \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, component \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m                          \u001b[39m.\u001b[39mformat(model_id, model_component_id, \u001b[39mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:398\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable.__call__\u001b[0;34m(self, request_iterator, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    392\u001b[0m              request_iterator: RequestIterableType,\n\u001b[1;32m    393\u001b[0m              timeout: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m              wait_for_ready: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m              compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 398\u001b[0m     response, ignored_call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_call(request_iterator,\n\u001b[1;32m    399\u001b[0m                                              timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    400\u001b[0m                                              metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    401\u001b[0m                                              credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[1;32m    402\u001b[0m                                              wait_for_ready\u001b[39m=\u001b[39;49mwait_for_ready,\n\u001b[1;32m    403\u001b[0m                                              compression\u001b[39m=\u001b[39;49mcompression)\n\u001b[1;32m    404\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:438\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable._with_call\u001b[0;34m(self, request_iterator, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exception:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    436\u001b[0m         \u001b[39mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n\u001b[0;32m--> 438\u001b[0m call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interceptor\u001b[39m.\u001b[39;49mintercept_stream_unary(continuation,\n\u001b[1;32m    439\u001b[0m                                                 client_call_details,\n\u001b[1;32m    440\u001b[0m                                                 request_iterator)\n\u001b[1;32m    441\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39mresult(), call\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/interceptors/base.py:69\u001b[0m, in \u001b[0;36mStreamMetadataInterceptorBase.intercept_stream_unary\u001b[0;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mintercept_stream_unary\u001b[39m(\u001b[39mself\u001b[39m, continuation, client_call_details, request):\n\u001b[1;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Invoked by gRPC when issuing a request using this interceptor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m _metadata_interceptor(continuation, client_call_details, request,\n\u001b[1;32m     70\u001b[0m                                  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value)\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/interceptors/base.py:48\u001b[0m, in \u001b[0;36m_metadata_interceptor\u001b[0;34m(continuation, client_call_details, request, key, value)\u001b[0m\n\u001b[1;32m     41\u001b[0m metadata\u001b[39m.\u001b[39mappend((key, value))\n\u001b[1;32m     42\u001b[0m client_call_details \u001b[39m=\u001b[39m _ClientCallDetails(\n\u001b[1;32m     43\u001b[0m     client_call_details\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m     44\u001b[0m     client_call_details\u001b[39m.\u001b[39mtimeout,\n\u001b[1;32m     45\u001b[0m     metadata,\n\u001b[1;32m     46\u001b[0m     client_call_details\u001b[39m.\u001b[39mcredentials,\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[39mreturn\u001b[39;00m continuation(client_call_details, request)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:425\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request_iterator)\u001b[0m\n\u001b[1;32m    420\u001b[0m (new_method, new_timeout, new_metadata, new_credentials,\n\u001b[1;32m    421\u001b[0m  new_wait_for_ready,\n\u001b[1;32m    422\u001b[0m  new_compression) \u001b[39m=\u001b[39m (_unwrap_client_call_details(\n\u001b[1;32m    423\u001b[0m      new_details, client_call_details))\n\u001b[1;32m    424\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     response, call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_thunk(new_method)\u001b[39m.\u001b[39;49mwith_call(\n\u001b[1;32m    426\u001b[0m         request_iterator,\n\u001b[1;32m    427\u001b[0m         timeout\u001b[39m=\u001b[39;49mnew_timeout,\n\u001b[1;32m    428\u001b[0m         metadata\u001b[39m=\u001b[39;49mnew_metadata,\n\u001b[1;32m    429\u001b[0m         credentials\u001b[39m=\u001b[39;49mnew_credentials,\n\u001b[1;32m    430\u001b[0m         wait_for_ready\u001b[39m=\u001b[39;49mnew_wait_for_ready,\n\u001b[1;32m    431\u001b[0m         compression\u001b[39m=\u001b[39;49mnew_compression)\n\u001b[1;32m    432\u001b[0m     \u001b[39mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[1;32m    433\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m rpc_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:452\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable.with_call\u001b[0;34m(self, request_iterator, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwith_call\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    445\u001b[0m     request_iterator: RequestIterableType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m     compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    451\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, grpc\u001b[39m.\u001b[39mCall]:\n\u001b[0;32m--> 452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_call(request_iterator,\n\u001b[1;32m    453\u001b[0m                            timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    454\u001b[0m                            metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    455\u001b[0m                            credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[1;32m    456\u001b[0m                            wait_for_ready\u001b[39m=\u001b[39;49mwait_for_ready,\n\u001b[1;32m    457\u001b[0m                            compression\u001b[39m=\u001b[39;49mcompression)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:438\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable._with_call\u001b[0;34m(self, request_iterator, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exception:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    436\u001b[0m         \u001b[39mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n\u001b[0;32m--> 438\u001b[0m call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interceptor\u001b[39m.\u001b[39;49mintercept_stream_unary(continuation,\n\u001b[1;32m    439\u001b[0m                                                 client_call_details,\n\u001b[1;32m    440\u001b[0m                                                 request_iterator)\n\u001b[1;32m    441\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39mresult(), call\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/interceptors/base.py:69\u001b[0m, in \u001b[0;36mStreamMetadataInterceptorBase.intercept_stream_unary\u001b[0;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mintercept_stream_unary\u001b[39m(\u001b[39mself\u001b[39m, continuation, client_call_details, request):\n\u001b[1;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Invoked by gRPC when issuing a request using this interceptor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m _metadata_interceptor(continuation, client_call_details, request,\n\u001b[1;32m     70\u001b[0m                                  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value)\n",
      "File \u001b[0;32m~/dynamite/client/octoml_profile/interceptors/base.py:48\u001b[0m, in \u001b[0;36m_metadata_interceptor\u001b[0;34m(continuation, client_call_details, request, key, value)\u001b[0m\n\u001b[1;32m     41\u001b[0m metadata\u001b[39m.\u001b[39mappend((key, value))\n\u001b[1;32m     42\u001b[0m client_call_details \u001b[39m=\u001b[39m _ClientCallDetails(\n\u001b[1;32m     43\u001b[0m     client_call_details\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m     44\u001b[0m     client_call_details\u001b[39m.\u001b[39mtimeout,\n\u001b[1;32m     45\u001b[0m     metadata,\n\u001b[1;32m     46\u001b[0m     client_call_details\u001b[39m.\u001b[39mcredentials,\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[39mreturn\u001b[39;00m continuation(client_call_details, request)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_interceptor.py:425\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request_iterator)\u001b[0m\n\u001b[1;32m    420\u001b[0m (new_method, new_timeout, new_metadata, new_credentials,\n\u001b[1;32m    421\u001b[0m  new_wait_for_ready,\n\u001b[1;32m    422\u001b[0m  new_compression) \u001b[39m=\u001b[39m (_unwrap_client_call_details(\n\u001b[1;32m    423\u001b[0m      new_details, client_call_details))\n\u001b[1;32m    424\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     response, call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_thunk(new_method)\u001b[39m.\u001b[39;49mwith_call(\n\u001b[1;32m    426\u001b[0m         request_iterator,\n\u001b[1;32m    427\u001b[0m         timeout\u001b[39m=\u001b[39;49mnew_timeout,\n\u001b[1;32m    428\u001b[0m         metadata\u001b[39m=\u001b[39;49mnew_metadata,\n\u001b[1;32m    429\u001b[0m         credentials\u001b[39m=\u001b[39;49mnew_credentials,\n\u001b[1;32m    430\u001b[0m         wait_for_ready\u001b[39m=\u001b[39;49mnew_wait_for_ready,\n\u001b[1;32m    431\u001b[0m         compression\u001b[39m=\u001b[39;49mnew_compression)\n\u001b[1;32m    432\u001b[0m     \u001b[39mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[1;32m    433\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m rpc_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_channel.py:1140\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable.with_call\u001b[0;34m(self, request_iterator, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwith_call\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m   1134\u001b[0m               request_iterator,\n\u001b[1;32m   1135\u001b[0m               timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m               wait_for_ready\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1139\u001b[0m               compression\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1140\u001b[0m     state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_blocking(request_iterator, timeout, metadata,\n\u001b[1;32m   1141\u001b[0m                                   credentials, wait_for_ready, compression)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/octoml/lib/python3.9/site-packages/grpc/_channel.py:1114\u001b[0m, in \u001b[0;36m_StreamUnaryMultiCallable._blocking\u001b[0;34m(self, request_iterator, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1111\u001b[0m _consume_request_iterator(request_iterator, state, call,\n\u001b[1;32m   1112\u001b[0m                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request_serializer, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1113\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1114\u001b[0m     event \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49mnext_event()\n\u001b[1;32m   1115\u001b[0m     \u001b[39mwith\u001b[39;00m state\u001b[39m.\u001b[39mcondition:\n\u001b[1;32m   1116\u001b[0m         _handle_event(event, state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_deserializer)\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:338\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:169\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:163\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: unexpected EOF while parsing\n",
      "ipdb> "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "result_file = 'sentence_transformer_eval_with_torch.csv'\n",
    "if not os.path.exists(result_file):\n",
    "    records = benchmark_all(model_ids, backends, result_file, 1)\n",
    "    records256 = benchmark_all(model_ids, backends, result_file, 256)\n",
    "\n",
    "    df = pd.DataFrame(data=records + records256)\n",
    "    df.to_csv(result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e0180-4bc0-465d-977f-3050d8af2e28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Analyze the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dff416-7412-4bf0-bbd8-1e63d2693906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model                          backend     time_ms  dollar_cost_per_million_requests  batch_size\n",
      "0         all-MiniLM-L12-v2             r6i.large/onnxrt-cpu   17.820951                          0.623733           1\n",
      "1         all-MiniLM-L12-v2        r6i.large/torch-eager-cpu   28.495958                          0.997359           1\n",
      "2         all-MiniLM-L12-v2             r7g.large/onnxrt-cpu   21.257491                          0.135812           1\n",
      "3         all-MiniLM-L12-v2          g4dn.xlarge/onnxrt-cuda   11.897796                          1.738400           1\n",
      "4         all-MiniLM-L12-v2      g4dn.xlarge/onnxrt-tensorrt   12.402284                          1.812111           1\n",
      "..                      ...                              ...         ...                               ...         ...\n",
      "65  paraphrase-MiniLM-L3-v2             r7g.large/onnxrt-cpu  872.684674                          5.575485         256\n",
      "66  paraphrase-MiniLM-L3-v2          g4dn.xlarge/onnxrt-cuda   60.543892                          8.846135         256\n",
      "67  paraphrase-MiniLM-L3-v2      g4dn.xlarge/onnxrt-tensorrt   89.447111                         13.069217         256\n",
      "68  paraphrase-MiniLM-L3-v2     g4dn.xlarge/torch-eager-cuda   73.714679                         10.770534         256\n",
      "69  paraphrase-MiniLM-L3-v2  g4dn.xlarge/torch-inductor-cuda   62.961038                          9.199307         256\n",
      "\n",
      "[70 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(result_file, index_col=0)\n",
    "# Rename here for more informative label display, altair does not\n",
    "# properly display `alt.X` in github.\n",
    "df.rename(columns={'cost_per_mreq': 'dollar_cost_per_million_requests'}, inplace=True)\n",
    "\n",
    "with pd.option_context('display.width', 120):\n",
    "    print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26ea9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('mimetype')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "alt.renderers.enable('mimetype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b8a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-af6eb9fe15b6262122ef517ddf5fdb37"
       },
       "datasets": {
        "data-af6eb9fe15b6262122ef517ddf5fdb37": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 43.8912479775,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1254.0356565
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 42.3355057055,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1209.5858773
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 21.316262162277773,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 3336.4584254
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.44170649911111,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 98.84057679999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.641416422555555,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 209.7131162
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.716172463444448,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 134.9395834
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.187976432111107,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 117.63634059999998
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 21.4132507155,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 611.8071633
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 23.1399270795,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 661.1407737000001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.838871327555555,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 1696.5189904000003
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 11.344228645444444,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 77.6411086
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.056245163388887,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 130.4229707
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.504651180388889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 99.2713769
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.618693507444446,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 86.3636818
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 77.7574467495,
          "model": "all-distilroberta-v1",
          "time_ms": 2221.6413357
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 75.23220327700002,
          "model": "all-distilroberta-v1",
          "time_ms": 2149.491522200001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 42.04370020586111,
          "model": "all-distilroberta-v1",
          "time_ms": 6580.753075699999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.751911882888887,
          "model": "all-distilroberta-v1",
          "time_ms": 114.65186839999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 29.25637804683333,
          "model": "all-distilroberta-v1",
          "time_ms": 200.2337661
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.28541693327778,
          "model": "all-distilroberta-v1",
          "time_ms": 111.4591273
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.914366378555554,
          "model": "all-distilroberta-v1",
          "time_ms": 108.91961779999998
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 74.27498704149998,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 2122.1424869
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 77.82040436449999,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 2223.4401247
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 39.39376047733335,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 6165.979900800002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.154765059166673,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 103.72082550000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.822140241222225,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 183.5735834
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.79133522288889,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 121.7657924
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.253941843722227,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 118.08781490000004
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.354051269,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 352.9728934
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.375842794000002,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 353.5955084
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 5.575485419138888,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 872.6846742999999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.8461353165,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.5438919
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.069216730055556,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.4471107
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.770533595444444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 73.7146786
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.199307189666666,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 62.96103779999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "row": {
         "field": "model",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "backend",
         "type": "nominal"
        }
       },
       "mark": "bar",
       "title": "Cheapest backend per model for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 256]).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    color='model',\n",
    "    row='model'\n",
    ").properties(title='Cheapest backend per model for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe3d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-af6eb9fe15b6262122ef517ddf5fdb37"
       },
       "datasets": {
        "data-af6eb9fe15b6262122ef517ddf5fdb37": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 43.8912479775,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1254.0356565
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 42.3355057055,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1209.5858773
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 21.316262162277773,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 3336.4584254
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.44170649911111,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 98.84057679999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.641416422555555,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 209.7131162
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.716172463444448,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 134.9395834
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.187976432111107,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 117.63634059999998
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 21.4132507155,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 611.8071633
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 23.1399270795,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 661.1407737000001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.838871327555555,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 1696.5189904000003
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 11.344228645444444,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 77.6411086
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.056245163388887,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 130.4229707
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.504651180388889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 99.2713769
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.618693507444446,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 86.3636818
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 77.7574467495,
          "model": "all-distilroberta-v1",
          "time_ms": 2221.6413357
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 75.23220327700002,
          "model": "all-distilroberta-v1",
          "time_ms": 2149.491522200001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 42.04370020586111,
          "model": "all-distilroberta-v1",
          "time_ms": 6580.753075699999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.751911882888887,
          "model": "all-distilroberta-v1",
          "time_ms": 114.65186839999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 29.25637804683333,
          "model": "all-distilroberta-v1",
          "time_ms": 200.2337661
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.28541693327778,
          "model": "all-distilroberta-v1",
          "time_ms": 111.4591273
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.914366378555554,
          "model": "all-distilroberta-v1",
          "time_ms": 108.91961779999998
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 74.27498704149998,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 2122.1424869
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 77.82040436449999,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 2223.4401247
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 39.39376047733335,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 6165.979900800002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.154765059166673,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 103.72082550000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.822140241222225,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 183.5735834
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.79133522288889,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 121.7657924
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.253941843722227,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 118.08781490000004
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.354051269,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 352.9728934
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.375842794000002,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 353.5955084
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 5.575485419138888,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 872.6846742999999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.8461353165,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.5438919
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.069216730055556,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.4471107
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.770533595444444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 73.7146786
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.199307189666666,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 62.96103779999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "row": {
         "field": "model",
         "type": "nominal"
        },
        "x": {
         "field": "time_ms",
         "type": "quantitative"
        },
        "y": {
         "field": "backend",
         "type": "nominal"
        }
       },
       "mark": "bar",
       "title": "Fastest backend per model for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 256]).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='time_ms',\n",
    "    color='model',\n",
    "    row='model',\n",
    ").properties(title='Fastest backend per model for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f468c60-f9da-4d8b-81ee-11d8cd881f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-9766763ced4986cc17ad1118f8dfa6b3"
       },
       "datasets": {
        "data-9766763ced4986cc17ad1118f8dfa6b3": [
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.44170649911111,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 98.84057679999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.641416422555555,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 209.7131162
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.716172463444448,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 134.9395834
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.187976432111107,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 117.63634059999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 11.344228645444444,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 77.6411086
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.056245163388887,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 130.4229707
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.504651180388889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 99.2713769
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.618693507444446,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 86.3636818
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.751911882888887,
          "model": "all-distilroberta-v1",
          "time_ms": 114.65186839999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 29.25637804683333,
          "model": "all-distilroberta-v1",
          "time_ms": 200.2337661
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.28541693327778,
          "model": "all-distilroberta-v1",
          "time_ms": 111.4591273
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.914366378555554,
          "model": "all-distilroberta-v1",
          "time_ms": 108.91961779999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.154765059166673,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 103.72082550000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.822140241222225,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 183.5735834
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.79133522288889,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 121.7657924
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.253941843722227,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 118.08781490000004
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.8461353165,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.5438919
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.069216730055556,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.4471107
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.770533595444444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 73.7146786
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.199307189666666,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 62.96103779999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "row": {
         "field": "model",
         "type": "nominal"
        },
        "x": {
         "field": "time_ms",
         "type": "quantitative"
        },
        "y": {
         "field": "backend",
         "type": "nominal"
        }
       },
       "mark": "bar",
       "title": "Fastest backend per model for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_256 = df[df['batch_size'] == 256]\n",
    "df_256_cpu = df_256[df_256['backend'].str.contains('cpu') == False]\n",
    "alt.Chart(df_256_cpu).mark_bar().encode(\n",
    "    y='backend',\n",
    "    x='time_ms',\n",
    "    color='model',\n",
    "    row='model',\n",
    ").properties(title='Fastest backend per model for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980fba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-af6eb9fe15b6262122ef517ddf5fdb37"
       },
       "datasets": {
        "data-af6eb9fe15b6262122ef517ddf5fdb37": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 43.8912479775,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1254.0356565
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 42.3355057055,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 1209.5858773
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 21.316262162277773,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 3336.4584254
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.44170649911111,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 98.84057679999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 30.641416422555555,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 209.7131162
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.716172463444448,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 134.9395834
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.187976432111107,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 117.63634059999998
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 21.4132507155,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 611.8071633
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 23.1399270795,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 661.1407737000001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.838871327555555,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 1696.5189904000003
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 11.344228645444444,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 77.6411086
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.056245163388887,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 130.4229707
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.504651180388889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 99.2713769
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.618693507444446,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 86.3636818
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 77.7574467495,
          "model": "all-distilroberta-v1",
          "time_ms": 2221.6413357
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 75.23220327700002,
          "model": "all-distilroberta-v1",
          "time_ms": 2149.491522200001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 42.04370020586111,
          "model": "all-distilroberta-v1",
          "time_ms": 6580.753075699999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.751911882888887,
          "model": "all-distilroberta-v1",
          "time_ms": 114.65186839999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 29.25637804683333,
          "model": "all-distilroberta-v1",
          "time_ms": 200.2337661
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.28541693327778,
          "model": "all-distilroberta-v1",
          "time_ms": 111.4591273
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.914366378555554,
          "model": "all-distilroberta-v1",
          "time_ms": 108.91961779999998
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 74.27498704149998,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 2122.1424869
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 77.82040436449999,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 2223.4401247
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 39.39376047733335,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 6165.979900800002
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.154765059166673,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 103.72082550000005
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 26.822140241222225,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 183.5735834
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.79133522288889,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 121.7657924
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.253941843722227,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 118.08781490000004
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.354051269,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 352.9728934
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.375842794000002,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 353.5955084
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 5.575485419138888,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 872.6846742999999
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.8461353165,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.5438919
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.069216730055556,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.4471107
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.770533595444444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 73.7146786
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.199307189666666,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 62.96103779999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "shape": {
         "field": "backend",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "time_ms",
         "type": "quantitative"
        }
       },
       "mark": {
        "size": 60,
        "type": "point"
       },
       "title": "Compare all model on time/cost for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 256]).mark_point(size=60).encode(\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    y='time_ms',\n",
    "    color='model',\n",
    "    shape='backend',\n",
    ").properties(title='Compare all model on time/cost for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f38a22-3aab-42b7-96db-dad176d4e3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-4754ddc4f2097985d5d711a3f6d7726f"
       },
       "datasets": {
        "data-4754ddc4f2097985d5d711a3f6d7726f": [
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.44170649911111,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 98.84057679999998
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.716172463444448,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 134.9395834
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.187976432111107,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 117.63634059999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 11.344228645444444,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 77.6411086
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 19.056245163388887,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 130.4229707
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 14.504651180388889,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 99.2713769
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 12.618693507444446,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 86.3636818
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.751911882888887,
          "model": "all-distilroberta-v1",
          "time_ms": 114.65186839999998
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 16.28541693327778,
          "model": "all-distilroberta-v1",
          "time_ms": 111.4591273
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.914366378555554,
          "model": "all-distilroberta-v1",
          "time_ms": 108.91961779999998
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 15.154765059166673,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 103.72082550000005
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.79133522288889,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 121.7657924
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 17.253941843722227,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 118.08781490000004
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 8.8461353165,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 60.5438919
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 13.069216730055556,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 89.4471107
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 10.770533595444444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 73.7146786
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 256,
          "dollar_cost_per_million_requests": 9.199307189666666,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 62.96103779999999
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "shape": {
         "field": "backend",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "scale": {
          "domain": [
           0,
           20
          ]
         },
         "type": "quantitative"
        },
        "y": {
         "field": "time_ms",
         "type": "quantitative"
        }
       },
       "mark": {
        "size": 60,
        "type": "point"
       },
       "title": "Compare all model on time/cost for batch size 256"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fast = df_256[df_256['dollar_cost_per_million_requests'] < 20]\n",
    "df_fast = df_fast[df_fast['time_ms'] < 200]\n",
    "alt.Chart(df_fast).mark_point(size=60).encode(\n",
    "    x=alt.X('dollar_cost_per_million_requests', scale=alt.Scale(domain=[0, 20])),\n",
    "    y='time_ms',\n",
    "    color='model',\n",
    "    shape='backend',\n",
    ").properties(title='Compare all model on time/cost for batch size 256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89345a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json",
       "config": {
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "name": "data-be210da5bd910291091f53a3993f80a5"
       },
       "datasets": {
        "data-be210da5bd910291091f53a3993f80a5": [
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.6237332955,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 17.8209513
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.997358544,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 28.4959584
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.1358117474166666,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 21.2574909
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.7384001202777777,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 11.8977955
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.8121114955555555,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 12.402284
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 2.6750035664444445,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 18.3080092
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 2.300394650944445,
          "model": "all-MiniLM-L12-v2",
          "time_ms": 15.7441459
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.3028990825,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 8.6542595
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.462517412,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 13.2147832
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.0664275649999999,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 10.397357999999995
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8748323293333332,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 5.9874456
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.9242441556666666,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 6.3256254
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.3791066298888892,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 9.4387526
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.1495810653333332,
          "model": "all-MiniLM-L6-v2",
          "time_ms": 7.867855199999999
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.7272780199999999,
          "model": "all-distilroberta-v1",
          "time_ms": 20.779371999999995
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.3410809440000004,
          "model": "all-distilroberta-v1",
          "time_ms": 38.31659840000001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.1731943291111111,
          "model": "all-distilroberta-v1",
          "time_ms": 27.1086776
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.028607874111111,
          "model": "all-distilroberta-v1",
          "time_ms": 7.0399018
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.120866199333333,
          "model": "all-distilroberta-v1",
          "time_ms": 7.6713276
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.6430633216111112,
          "model": "all-distilroberta-v1",
          "time_ms": 11.2453003
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.3228454507222225,
          "model": "all-distilroberta-v1",
          "time_ms": 9.0536951
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.6680220364999998,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 19.0863439
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.0219203295,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 29.1977237
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.1828929371666666,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 28.6267206
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.7765972122222223,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 5.315114
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8773952351111112,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 6.0049864
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.4101408344444444,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 9.651154
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.0274821172222224,
          "model": "paraphrase-albert-small-v2",
          "time_ms": 7.032197
         },
         {
          "backend": "r6i.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.188386331,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 5.3824666
         },
         {
          "backend": "r6i.large/torch-eager-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.273279048,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 7.807972800000001
         },
         {
          "backend": "r7g.large/onnxrt-cpu",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.0426667281944444,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 6.6782705
         },
         {
          "backend": "g4dn.xlarge/onnxrt-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.6944918705,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 4.7531763
         },
         {
          "backend": "g4dn.xlarge/onnxrt-tensorrt",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.7333439838333334,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 5.019084299999999
         },
         {
          "backend": "g4dn.xlarge/torch-eager-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 1.0003627115000002,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 6.8465889
         },
         {
          "backend": "g4dn.xlarge/torch-inductor-cuda",
          "batch_size": 1,
          "dollar_cost_per_million_requests": 0.8528192001111112,
          "model": "paraphrase-MiniLM-L3-v2",
          "time_ms": 5.8367854
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "model",
         "type": "nominal"
        },
        "shape": {
         "field": "backend",
         "type": "nominal"
        },
        "x": {
         "field": "dollar_cost_per_million_requests",
         "type": "quantitative"
        },
        "y": {
         "field": "time_ms",
         "type": "quantitative"
        }
       },
       "mark": {
        "size": 60,
        "type": "point"
       },
       "title": "Compare all model on time/cost for batch size 1"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df['batch_size'] == 1]).mark_point(size=60).encode(\n",
    "    x='dollar_cost_per_million_requests',\n",
    "    y='time_ms',\n",
    "    color='model',\n",
    "    shape='backend',\n",
    ").properties(title='Compare all model on time/cost for batch size 1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c66a465a096ad5eb721a6dce4571ab61f8846383e6c54b37279c5c4bd238d3f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
